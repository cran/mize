<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="James Melville" />

<meta name="date" content="2026-02-08" />

<title>Mize</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Mize</h1>
<h4 class="author">James Melville</h4>
<h4 class="date">2026-02-08</h4>



<p><code>mize</code> is a package for doing unconstrained numerical
optimization. This vignette describes the use of the <code>mize</code>
function, which takes as input a function to optimize, its gradient and
a starting point and returns the optimized parameters.</p>
<p>First, I’ll describe the required structure of the function and
gradient to optimize, then the various convergence options that apply no
matter what optimization methods are being used.</p>
<p>In the last section, I’ll describe the methods which are available
and any options that apply to them specifically.</p>
<div id="the-fg-list" class="section level2">
<h2>The <code>fg</code> list</h2>
<p>If you look at <code>stats::optim</code>, you’ll see you pass the
function to be optimized and its gradient separately. By contrast,
<code>mize</code> asks you to bundle them up in a list, which I lazily
refer to as an <code>fg</code> list. Here, for example, is the venerable
Rosenbrock function:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>rb_fg <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>   <span class="at">fn =</span> <span class="cf">function</span>(x) { <span class="dv">100</span> <span class="sc">*</span> (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>]) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> x[<span class="dv">1</span>]) <span class="sc">^</span> <span class="dv">2</span>  },</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>   <span class="at">gr =</span> <span class="cf">function</span>(x) { <span class="fu">c</span>( <span class="sc">-</span><span class="dv">400</span> <span class="sc">*</span> x[<span class="dv">1</span>] <span class="sc">*</span> (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>]) <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> x[<span class="dv">1</span>]),</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>                          <span class="dv">200</span> <span class="sc">*</span>        (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>])) })</span></code></pre></div>
<p>For minimizing a simple function, there’s not a lot of advantage to
this. But for more complex optimizations, there are some advantages:</p>
<ul>
<li>If <code>fn</code> and <code>gr</code> require extra
parameterization, you can store the extra state in a function that
creates and returns the <code>fg</code> list, and then <code>fn</code>
and <code>gr</code> can access that state via the created closure.
<code>stats::optim</code> takes an alternative approach where extra data
is passed as <code>...</code> to the <code>optim</code> function and
then internally this is passed to <code>fn</code> and <code>gr</code>
when needed.</li>
<li>For some optimization methods (e.g. those using a strong Wolfe line
search), both <code>fn</code> and <code>gr</code> need to be evaluated
for the same value of <code>par</code>. If there’s a lot of shared work
between <code>fn</code> and <code>gr</code>, this can be quite
inefficient. If you provide an optional routine called <code>fg</code>,
which calculates the function and gradient simultaneously, this will be
used by some parts of <code>mize</code> in preference to calling
<code>fn</code> and <code>gr</code> separately, which can save a bit of
time.</li>
</ul>
<p>A version of <code>rb_fg</code> with an optional <code>fg</code>
function could look like:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>rb_fg <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>   <span class="at">fn =</span> <span class="cf">function</span>(x) { <span class="dv">100</span> <span class="sc">*</span> (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>]) <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> x[<span class="dv">1</span>]) <span class="sc">^</span> <span class="dv">2</span>  },</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>   <span class="at">gr =</span> <span class="cf">function</span>(x) { <span class="fu">c</span>( <span class="sc">-</span><span class="dv">400</span> <span class="sc">*</span> x[<span class="dv">1</span>] <span class="sc">*</span> (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>]) <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> x[<span class="dv">1</span>]),</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>                          <span class="dv">200</span> <span class="sc">*</span>        (x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>])) },</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>   <span class="at">fg =</span> <span class="cf">function</span>(x) {   </span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>     a <span class="ot">&lt;-</span> x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>] <span class="sc">*</span> x[<span class="dv">1</span>]</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>     b <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> x[<span class="dv">1</span>]</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>     <span class="fu">list</span>( </span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>       <span class="at">fn =</span> <span class="dv">100</span> <span class="sc">*</span> a <span class="sc">^</span> <span class="dv">2</span> <span class="sc">+</span> b <span class="sc">^</span> <span class="dv">2</span>,</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>       <span class="at">gr =</span> <span class="fu">c</span>( <span class="sc">-</span><span class="dv">400</span> <span class="sc">*</span> x[<span class="dv">1</span>] <span class="sc">*</span> a <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> b,</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>                <span class="dv">200</span> <span class="sc">*</span> a)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>     )</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>   }</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>)</span></code></pre></div>
<p>In the case of the Rosenbrock function, I’m not suggesting you should
actually do this. See the vignette on <a href="mmds.html">Metric
Multi-Dimensional Scaling</a> for the sort of scenario where it could
make a difference.</p>
<p>For the rest of this vignette, we’ll just assume you’re only
providing <code>fn</code> and <code>gr</code>.</p>
</div>
<div id="the-parameters-to-optimize" class="section level2">
<h2>The parameters to optimize</h2>
<p>The parameters to optimize as passed as a numeric vector,
<code>par</code>.</p>
<p>As well as using the <code>rb_fg</code> list we defined above to
describe the Rosenbrock function, we’ll also start from the following
starting point:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>rb0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.2</span>, <span class="dv">1</span>)</span></code></pre></div>
</div>
<div id="basic-options" class="section level2">
<h2>Basic options</h2>
<p>If you don’t particularly care about how your function gets
optimized, the default setting is to use the limited-memory
Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) optimizer, which is a good
choice for pretty much anything.</p>
<p>You may still need to tweak some options that are independent of the
method, like the maximum number of iterations to use, or other
convergence criteria.</p>
<div id="defaults" class="section level3">
<h3>Defaults</h3>
<p>By default, <code>mize</code> will grind away on the function you
give it for up to 100 iterations. But it will give up early if it senses
that no further progress is being made.</p>
<p>The return value provides both the optimized parameter as well as
some information about how much work was done to get there.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co"># What were the final parameter values? (should be close to c(1, 1))</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>res<span class="sc">$</span>par</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.9999972 0.9999923</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># What was the function value at that point (should be close to 0)</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>res<span class="sc">$</span>f</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co">#&gt; [1] 4.521673e-10</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># How many iterations did it take?</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>res<span class="sc">$</span>iter</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co">#&gt; [1] 37</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="co"># How many function evaluations?</span></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>res<span class="sc">$</span>nf</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="co">#&gt; [1] 49</span></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a><span class="co"># How many gradient evaluations?</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>res<span class="sc">$</span>ng</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="co">#&gt; [1] 49</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a><span class="co"># Why did the optimization terminate?</span></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>res<span class="sc">$</span>terminate</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a><span class="co">#&gt; $what</span></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a><span class="co">#&gt; [1] &quot;abs_tol&quot;</span></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a><span class="co">#&gt; $val</span></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a><span class="co">#&gt; [1] 1.06609e-08</span></span></code></pre></div>
<p>As you can see from looking at <code>res$iter</code>, the L-BFGS
didn’t use all 100 iterations it had available. The difference between
consecutive function values, <code>res$f</code> got sufficiently low,
that the absolute tolerance criterion was passed:
<code>res$terminate$what</code> will contain a string code that
communicates what happened, while <code>res$terminate$val</code>
provides the specific value that causes the termination, although it’s
rarely interesting.</p>
<p>There is an entire vignette on the <a href="convergence.html">different convergence criteria</a> available.
The defaults should do a reasonable, albeit fairly conservative,
job.</p>
</div>
<div id="logging-progress-to-console" class="section level3">
<h3>Logging progress to console</h3>
<p>If you want to keep track of the progress of the optimization, set
the <code>verbose</code> option to <code>TRUE</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">grad_tol =</span> <span class="fl">1e-3</span>, <span class="at">ginf_tol =</span> <span class="fl">1e-3</span>, <span class="at">max_iter =</span> <span class="dv">10</span>, </span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>            <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 0 f = 24.2 g2 = 232.9 ginf = 215.6 nf = 1 ng = 1 step = 0 alpha = 0</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 1 f = 19.5 g2 = 200.9 ginf = 185.6 nf = 3 ng = 3 step = 0.02169 alpha = 9.312e-05</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 2 f = 11.57 g2 = 135.4 ginf = 124.6 nf = 4 ng = 4 step = 0.04729 alpha = 0.3469</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 3 f = 4.281 g2 = 17.28 ginf = 16.27 nf = 5 ng = 5 step = 0.09809 alpha = 1</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 4 f = 4.144 g2 = 2.588 ginf = 2.469 nf = 6 ng = 6 step = 0.01426 alpha = 1</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 5 f = 4.139 g2 = 1.773 ginf = 1.626 nf = 7 ng = 7 step = 0.002565 alpha = 1</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 6 f = 4.132 g2 = 2.44 ginf = 2.327 nf = 8 ng = 8 step = 0.004716 alpha = 1</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 7 f = 4.12 g2 = 3.727 ginf = 3.033 nf = 9 ng = 9 step = 0.009086 alpha = 0.4616</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 8 f = 4.098 g2 = 5.427 ginf = 3.863 nf = 10 ng = 10 step = 0.01724 alpha = 0.2631</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 9 f = 2.569 g2 = 8.248 ginf = 7.431 nf = 16 ng = 16 step = 0.8311 alpha = 4.743</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 10 f = 2.553 g2 = 11.67 ginf = 9.996 nf = 18 ng = 18 step = 0.04657 alpha = 0.05068</span></span></code></pre></div>
<p>As you can see, information about the state of the optimization is
logged to the console every time the convergence criteria are checked.
The values logged are:</p>
<ul>
<li><code>f</code> - The current value of the function.</li>
<li><code>g2</code> - The current value of the gradient 2-norm (if you
set a non-<code>NULL</code> <code>grad_tol</code>).</li>
<li><code>ginf</code> - The current value of the gradient infinity norm
(if you set a non-<code>NULL</code> <code>ginf_tol</code>).</li>
<li><code>nf</code> - The total number of function evaluations so
far.</li>
<li><code>ng</code> - The total number of gradient evaluations so
far.</li>
<li><code>step</code> - The magnitude of the change between the previous
version of <code>par</code> and the current value.</li>
</ul>
<p>Note that values are also provided for iteration 0, which represents
the state of <code>par</code> at its starting point. Because this
requires a function evaluation that might otherwise not be needed, you
may get results where <code>nf</code> and <code>ng</code> are one larger
than you’d otherwise expect if <code>verbose</code> was
<code>FALSE</code>.</p>
<p>Logging every iteration might be more output than you want. You can
also set the <code>log_every</code> parameter to make logging slightly
less noisy:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">grad_tol =</span> <span class="fl">1e-3</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>, <span class="at">log_every =</span> <span class="dv">10</span>)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 0 f = 24.2 g2 = 232.9 nf = 1 ng = 1 step = 0 alpha = 0</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 10 f = 2.553 g2 = 11.67 nf = 18 ng = 18 step = 0.04657 alpha = 0.05068</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 20 f = 0.5142 g2 = 3.001 nf = 31 ng = 31 step = 0.1319 alpha = 1</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 30 f = 0.009862 g2 = 3.333 nf = 42 ng = 42 step = 0.03554 alpha = 0.1706</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#&gt; 23:11:10 iter 37 f = 4.522e-10 g2 = 0.0009379 nf = 49 ng = 49 step = 0.0002131 alpha = 1</span></span></code></pre></div>
<p>On the assumption that you probably want to see information about the
last iteration, this is also reported, even if it wouldn’t normally be
reported according to the setting for <code>log_every</code>.</p>
</div>
<div id="storing-progress" class="section level3">
<h3>Storing progress</h3>
<p>You may wish to do more than stare at the progress numbers logged to
the console during optimization when <code>verbose = TRUE</code>. If you
would like access to these values after optimization, set the
<code>store_progress</code> parameter to true, and a data frame called
<code>progress</code> will be part of the return value:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">store_progress =</span> <span class="cn">TRUE</span>, <span class="at">log_every =</span> <span class="dv">10</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>res<span class="sc">$</span>progress</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co">#&gt;               f nf ng         step      alpha</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co">#&gt; 0  2.420000e+01  1  0 0.0000000000 0.00000000</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co">#&gt; 10 2.552598e+00 18 18 0.0465699833 0.05067737</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co">#&gt; 20 5.142086e-01 31 31 0.1319347626 1.00000000</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co">#&gt; 30 9.862134e-03 42 42 0.0355364981 0.17057755</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co">#&gt; 37 4.521673e-10 49 49 0.0002130984 1.00000000</span></span></code></pre></div>
<p>As can be seen, <code>store_progress</code> will also obey the
<code>log_every</code> parameter and not store every iteration of
convergence information if requested.</p>
</div>
</div>
<div id="optimization-methods" class="section level2">
<h2>Optimization Methods</h2>
<p>The available optimization methods are listed below. I won’t be
describing the algorithms behind them in any depth. For that, you should
look in the book by <a href="https://doi.org/10.1007/978-0-387-40065-5">Nocedal and
Wright</a>.</p>
<p>Many of these methods can be thought of as coming up with a direction
to move in, but with the distance to travel in that direction as being
someone else’s problem, i.e. a line search method needs to also be used
to determine the step size. We’ll discuss the available step size
methods separately.</p>
<div id="steepest-descent-sd" class="section level3">
<h3>Steepest Descent (<code>&quot;SD&quot;</code>)</h3>
<p>The good news is that you always get some improvement with steepest
descent. The bad news is everything else: it often takes a long time to
make any progress, unless you are <a href="https://doi.org/10.1093/imanum/8.1.141">very clever with the step
size</a>. It is however, the basis for most other gradient descent
techniques, and many of them will revert back to this direction, at
least temporarily, if things start going wrong.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;SD&quot;</span>)</span></code></pre></div>
</div>
<div id="broyden-fletcher-goldfarb-shanno-bfgs" class="section level3">
<h3>Broyden-Fletcher-Goldfarb-Shanno (<code>&quot;BFGS&quot;</code>)</h3>
<p>The BFGS method is a quasi-Newton method: it constructs an
approximation to the inverse of the Hessian. This saves on the annoyance
of working out what the second derivatives are for your problem, as well
as on the time it takes to calculate them on every iteration, but not on
the storage cost, which is O(N^2). It’s therefore not appropriate for
large problems.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>)</span></code></pre></div>
<p>By default, after the first iteration, the inverse Hessian
approximation is scaled to make it more similar to the real inverse
Hessian (in terms of eigenvalues). If you don’t want to do this, set the
<code>scale_hess</code> parameter to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">scale_hess =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
</div>
<div id="limited-memory-bfgs-l-bfgs" class="section level3">
<h3>Limited-Memory BFGS (<code>&quot;L-BFGS&quot;</code>)</h3>
<p>The L-BFGS method builds the inverse Hessian approximation
implicitly, yielding something close to the BFGS direction without ever
directly creating a matrix. This makes it suitable for larger problems.
The closeness to the BFGS result is determined by the size of
<code>memory</code> parameter, which determines the number of previous
updates that are used to create the direction for the next iteration.
The higher this value, the closer to the BFGS result will be obtained,
but the more storage is required. In practice, the recommended
<code>memory</code> size is between 3 and 7. The default is 5.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;L-BFGS&quot;</span>, <span class="at">memory =</span> <span class="dv">7</span>)</span></code></pre></div>
<p>The L-BFGS method also applies a similar Hessian scaling method as
with the BFGS method, but does so at every iteration. If you don’t want
it for some reason, you can also set <code>scale_hess</code> to
<code>FALSE</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;L-BFGS&quot;</span>, <span class="at">scale_hess =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
</div>
<div id="conjugate-gradient-cg" class="section level3">
<h3>Conjugate Gradient (<code>&quot;CG&quot;</code>)</h3>
<p>When someone is pulling a decent default optimization off the shelf,
if they’re not reaching for L-BFGS, they’re probably going for a
conjugate gradient method. In their favor, they require less storage
than the L-BFGS. However, they tend to be a lot more sensitive to poor
scaling of the function (i.e. when changing different parameters causes
very different changes to the objective function) and so need tighter
line search criteria to make progress.</p>
<p>There are in fact multiple different methods out there under the name
‘conjugate gradient’, which differ in how the direction is determined
from the current and previous gradient and the previous direction. The
default method in <code>mize</code> is the “PR+” method (sometimes
referred to as “PRP+”), which refers to its inventors, Polak and Ribiere
(the third “P” is Polyak who independently discovered it). The “+”
indicates that a restart to steepest descent is included under certain
conditions, which prevents the possibility of convergence failing.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>)</span></code></pre></div>
<p>Other CG update methods are available via the <code>cg_update</code>
parameter. Methods which seem competitive or even superior to the
<code>&quot;PR+&quot;</code> update is the <code>&quot;HS+&quot;</code> method (named after
Hestenes and Stiefel) and the <code>&quot;HZ+&quot;</code> update (named after
Hager and Zhang, and part of the <a href="https://users.clas.ufl.edu/hager/papers/Software/">CG_DESCENT</a>
optimizer):</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">cg_update =</span> <span class="st">&quot;HZ+&quot;</span>)</span></code></pre></div>
</div>
<div id="nesterov-accelerated-gradient-nag" class="section level3">
<h3>Nesterov Accelerated Gradient (<code>&quot;NAG&quot;</code>)</h3>
<p>The Nesterov Accelerated Gradient method consists of doing a steepest
descent step immediately followed by a step that resembles momentum (see
below), but with the previous gradient term replaced with the current
gradient.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;NAG&quot;</span>)</span></code></pre></div>
<p>Be aware that the convergence of NAG is not guaranteed to be
monotone, that is you may (and probably will) see the function value
increase between iterations. Getting the optimum convergence rates
guaranteed by theory requires you to be able to estimate how strong the
convexity of the function you are minimizing is.</p>
<p>To see this in action, let’s increase the iteration count to 100 and
plot the progress of the function:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;NAG&quot;</span>, <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="fu">plot</span>(res<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(res<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAAYFBMVEUAAAAAADoAAGYAOpAAZrY6AAA6ADo6kNtmAABmADpmAGZmOpBmkJBmtrZmtv+QOgCQkGaQtpCQ2/+2ZgC2tma2/7a2///bkDrb25Db/7bb////tmb/25D//7b//9v///8OZx2zAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAKDUlEQVR4nO2d62LbKBCFlcRu3O2u04231W4cJ+//litAki8SHC4jQM45P1I7GQ3SVxgQoFHzSTnVlD6B2kVAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAR0BnRsjPYFz6ZC9YBOu2ZjPn28NI+/y51PdTKATn9cMrn+9sXFGATU16Dd/vT9tfCp1KkB0DMBzatvYm0ziiH6SmMMYg2aF4M00ADo8PRmGhrHidcaYtBD18CO3Y/3LQldaejFNmoMrQbTuipRowygQ3OpZ6t1dx/i7uma1SgMUDcOMi3MXYPagd3RBnE1MT8Q0Ofh8bdpYdZL/1T1Z/xba8F4t4BUIzP9mJ2PuiEZPh4tjex+AfmoihokVMIigLr61VehgjGoHCAVqFvT0Kw67UwXYDVaHFAjVEQEoK77et9uPg+bLOXGF1AKkIrAx64J2cKv20vw8CJWZQF1nf1nmzTdsTSgRqqImCa2Oe2e3vRNR4ZyE/yLlBEVpJuHVzNatGgI0Up5x0Fj2y0JyEMfL6j9LQKoGbGcfyT7FDfUclawcHd+mlQeiUIWGgfpfk6k3AA153+LAqp1HHR2eRmI5Lz6GiaNg8LLjXFZHlCN46ALj00z/Z2AW1/DSsdB9QDC4yDJcqMclgUkIgLK6+7GX2FAXRt7ejs4plwly03yVwbQ8eG1VUE6htBi0x0VAVITzmqmua5uviJAahykAFU1ULS4K1qD0paeM8X89GKiY1Cbtr3jngGZ+bCHtG1UsoCs3soAktAdA7pYNs1Rbpq3EoAuFt5zlJvmrUyQltjeKgrI7qxMDXKvVwiXm+aMQRo4qxaQmnJ83zoGA3cM6GJV0BqtNZ9vr46ILnDm5/veugD1e36Ozd56Q6//YFY9ltpAdTEdUBugYbJV3W5Yrl7Vm364dNPlyUx3XB9eG6Ch1XSXbu3wVe1pl6pBt3BrAzSMpFUNsgE67dRm2HHHcEq5+MjaAJ1jkGtlo3/y12oQe+YzbbM6QKYf6yrPIWGoGHnmc4fVB0hCce5mjyIgcNBCgPrGHANIDZFLTJhZJ54XABS+en0bpAtMueYDdDUM9T1m+HDRzceVHlYuPkQc0M0w1Peo4cPFQDGm9NBy8RHCgG4GEqupQS4IgoAm46zVxKA8gOaGoeGHlujFHPZygOZuoVcyDnKZSwGan2GIj0FRip3ucAJy1i7vEqyDCF8Hw4cSyz5OawlAdh8xQTr7sg8wdre/xAJialD2ZZ+lAaUTDjGUd4dsU6/P3YA9HIQZyrtbGJDbJHrCLDFzRwAgaJoGKKH9WgzNPHObNlIMmGxZFlBK9Zw3/Hgx3bzEFrzkCAEtkq8/ohfrU3RJ3M0XB5TcAmcMhxokcTdfGlByCJ81NA8TitzNy5xgNCCZ/59bQ49Hmr3dlQUk00UEG2qNWeBsi68VAEq+C4k0VFJjAPPEnRuQTCcSB8jzgoIB+aQJNDMiHy+OJzqKA/K+z/e0Gwy90gQOMyKHyf6G2/mgYoDkW44x9EsTOM6pHTa11qClAHmmCRywdD1eGqClZiv8I2pwDfJKEziOkqxJPMoCkrxVvjH0ShPo7w4ldEyeZJl1IDnXMjH0SRMYU+4sp/RJqPyAhGTvfi8pLQMoyGt1gPQf5ZJvlADklR4ntVyDaBFAYU4jAOVJj3POJpWkAoBypcdxPmKQUE6g0zhAedLjiGQfmHaXicdjw2zpcRYBFOozKkhnSo8jk7+icX4NPVzAUNDdAoCCXWYGlC+X60WZ1i/BR3sa1pgex7eoHIBS0uOElyugZvZj+MGehnWmx/EsKwegKtPjeJYVUWp8DaoqPY5nYVkAVZkex7OwPIAqTI/jWVhMoVUPFKULywPIa5+059JzLiXMv8X1YkjeS8+ZlBWQR//uv/ScSU18mTE1CO59sS89h5cro/gJ3EWCtP/Scy5VBsh/6TmXsgHyfRm2bem5xHSHLvcz9v8ktAZJvU79bgF9jmk58K2Ga8R0z4C8VREgVR4BuRS9yJYwDoKN7IsC8shh1uuLAvLIYSZZrpCixxXxN6uuHGaC5QopIyCfHGaC5ZZWSgxKWX6+Z0Alc5jl15eYck1RMUCrUQQgiewvE6cCZsLuAqvEbZBOXBcLOod1AZLJQBV0DusCJJPDLOgc1gWINQiZMwYhc/ZiAuaiTglIqOz7A3RP6tfFRNIu3KVYg4AICIiAgAgIiICACAiIgIAICIiAgAgIiICA5AEdXZNKHy9Nn59oNHPaI6k5rI2cuxmJA1JZiKyvH/t46f7SqksazZz2sLCnN7P1X8bdnKQBmaltW34LkyGt7V/jpszc9l6FibmblTSgEYHDpvsfHs187O2FfXu9KTXJ3azEAelzdq8dHboLGcx87K06Pv7a6ZAm425W0oBM+3dGAbV8Mpp52NvVqtk9tVtHxt2s8gM6DjFaANBDX19WBAjWcbP8JtMmTKzp4s6KmhiKkn0WOZmoakB0UFYUpEE/e34aRqJfNvsJjqvq5t0jtfft861Z0shObSQY34y7joGifvDXeoKtWVlSfx/NXPZQx+HORcbdjHizCkRAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQDUAOv1If1Rfzs2NKgB0aBxvLo1304p4rQDQ4fHXj38d7weOdJP4BpVRxQG9b/dd20h7WcWcG58M6z7KCOj0/afadNn2TUEnetj3V6a+f/t7q//S2407W7q28/Dz4XX+8Imb0+6vXfcL9WuRfWY5Ae30m1z1G7efzSY8ndby6T8DaNuYlwgaO/NpozcNd186QLOHT9ycdmozvzJfXw1SUcGEhstkjcPjFmb32fD+JbOXbtxyflBXPHf4xI22UPVpjYD2ww5dfQFjCzjp3eCGxLH/vzffup+GwPDryeETN9qs+7FaQH1+h7158EdfhIqu4w7eK0Ddx/Ya0OTwWzerB3S1gfCgv3VXprbyzgCy1KDrw6/drB3Q9Xl339Sl/3jTwUbv4FUxaD+JQUPUnRw+cbN2QObxgUP/0IC+oK77+a/rrt63agev7rt0MJr2YnOHT9ysHpAeyKhmc+z3A+sgslHN6c+tjia93eU46PGfPrhMD791s2JAVvUDRdfths/TF3d7s9rLAkj/WurGKkLVAzI9u0xziVE9gCoVAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREBABAREQEAEBERAQAQEREND/6sgc6+b84HIAAAAASUVORK5CYII=" /><!-- --></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>res<span class="sc">$</span>f</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.04420106</span></span></code></pre></div>
<p>Yikes. If you let this optimization continue, it does eventually
settle down, but it’s a real test of patience. Clearly, in this case,
there’s probably a better estimate of the strong convexity
parameter.</p>
<p>You can control the strong convexity estimate with the
<code>nest_q</code> parameter, which is inversely related to the amount
of momentum that is applied. It should take a value between
<code>0</code> (you think your function is strongly convex) and
<code>1</code> (method becomes steepest descent). The default is zero,
and this is in fact the assumed value in most applications and
papers.</p>
<p>Let’s try turning down the momentum a little bit and repeat:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>resq <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;NAG&quot;</span>, <span class="at">nest_q =</span> <span class="fl">0.001</span>, </span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>            <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="fu">plot</span>(res<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(res<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">log</span>(res<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="fu">log</span>(resq<span class="sc">$</span>progress<span class="sc">$</span>f)))</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a><span class="fu">lines</span>(resq<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resq<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>resq<span class="sc">$</span>f</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="co">#&gt; [1] 1.147505</span></span></code></pre></div>
<p>The red line is the result with <code>nest_q = 0.001</code>. That’s
better. Unfortunately, I have no advice on how you should go about
estimating the right value for <code>nest_q</code>. However, see the
section on Adaptive Restart for ways to ameliorate the problem.</p>
<p>To get the theoretical guarantees about convergence, you actually
need to follow a fairly strict formula about what the step size should
be (it’s related to the reciprocal of the Lipschitz constant of the
function), and <a href="https://hdl.handle.net/1807/36012">Sutskever</a>
notes that for non-convex functions (with sparse stochastic gradient)
the theoretically optimal momentum and step size values are too
aggressive. Alas, the best advice (at least for deep learning
applications) given was “manual tuning”.</p>
<p>There are connections between NAG and classical momentum, and there
is some evidence (at least in deep learning) that replacing classical
momentum with the NAG method, but using the same momentum schedule (see
below) can improve results. I have written more (much more) on the
subject <a href="https://jlmelville.github.io/mize/nesterov.html">here</a>.</p>
</div>
<div id="classical-momentum-mom" class="section level3">
<h3>Classical Momentum (<code>&quot;MOM&quot;</code>)</h3>
<p>Momentum methods use steepest descent and then add a fraction of the
previous iteration’s direction. Often the effectiveness of this approach
is likened to inertia and has the effect of damping the oscillation in
direction that slows the progress of steepest descent with poorly-scaled
functions.</p>
<p>Originally popular for training neural networks, the momentum value
is often a constant and set to a fairly high value, like
<code>0.9</code>. You can set a constant momentum by setting the
<code>mom_schedule</code> parameter:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>)</span></code></pre></div>
<p>Momentum-based methods share the same issues as Nesterov Accelerated
Gradient: convergence need not be monotone. The plot below
demonstrates:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>,</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>            <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="fu">plot</span>(res<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(res<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>res<span class="sc">$</span>f</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="co">#&gt; [1] 2.414765</span></span></code></pre></div>
<p>To be fair, there is little reason to believe that the sort of
settings used for neural network training would be effective for
minimizing the Rosenbrock function.</p>
<p>Non-constant momentum schedules are also available, by passing a
string to <code>mom_schedule</code>. These include:</p>
<ul>
<li>A <code>switch</code> function, that steps the momentum from one
value (<code>mom_init</code>), to another (<code>mom_final</code>) at a
specified iteration (<code>mom_switch_iter</code>):</li>
</ul>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="co"># Switch from a momentum of 0.4 to 0.8 at iteration 5</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="st">&quot;switch&quot;</span>,</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>            <span class="at">mom_init =</span> <span class="fl">0.4</span>, <span class="at">mom_final =</span> <span class="fl">0.8</span>, <span class="at">mom_switch_iter =</span> <span class="dv">5</span>)</span></code></pre></div>
<ul>
<li>A <code>ramp</code> function, the linearly increases the momentum
from <code>mom_init</code> to <code>mom_final</code> over
<code>max_iter</code> iterations:</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="st">&quot;ramp&quot;</span>,</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>            <span class="at">mom_init =</span> <span class="fl">0.4</span>, <span class="at">mom_final =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<ul>
<li>The schedule used in NAG can be used here by asking for a
<code>&quot;nsconvex&quot;</code> schedule.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="st">&quot;nsconvex&quot;</span>)</span></code></pre></div>
<ul>
<li>The <code>nest_q</code> parameter can also be used here as it can be
with NAG:</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="st">&quot;nsconvex&quot;</span>,</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>            <span class="at">nest_q =</span> <span class="fl">0.001</span>)</span></code></pre></div>
<p>If none of this meets your needs, you can pass a function that takes
the current iteration number and the maximum number of iterations as
input and returns a scalar, which will be used as the momentum value,
although the value will be clamped between 0 and 1.</p>
<p>For example, you could create a momentum schedule that randomly picks
a value between 0 and 1:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>mom_fn <span class="ot">&lt;-</span> <span class="cf">function</span>(iter, max_iter) {</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>  <span class="fu">runif</span>(<span class="at">n =</span> <span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>}</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> mom_fn)</span></code></pre></div>
<p>I’m not saying it’s a <em>good</em> idea.</p>
</div>
<div id="simplified-nesterov-momentum" class="section level3">
<h3>Simplified Nesterov Momentum</h3>
<p>Interest in NAG in the neural network community (specifically deep
learning), was sparked by papers from <a href="https://proceedings.mlr.press/v28/sutskever13.html">Sutskever and
co-workers</a> and <a href="https://arxiv.org/abs/1212.0901">Bengio and
co-workers</a> who showed that it could be related to classical momentum
in a way that made it easy to implement if you already had a working
classical momentum implementation, and more importantly, that it seemed
to give improvements over classical momentum.</p>
<p>As <code>mize</code> already offers NAG by setting
<code>method = &quot;NAG&quot;</code>, being able to use it in a different form
related to classical momentum may seem superfluous. But in fact, the use
of what’s often called “NAG” in deep learning is slightly more general
(and as far as I’m aware, has no convergence theory to go with it, so
improvements are not guaranteed). It’s probably better to refer to it as
(simplified) Nesterov momentum.</p>
<p>At any rate, the key observation of Sutskever was that NAG could be
considered a momentum method where the momentum stage occurs before the
gradient descent stage. In this formulation, you can have whatever
momentum schedule you like, and the hope is that the gradient descent
stage can more easily correct if the momentum schedule is too aggressive
because it gets to “see” the result of the momentum update and hence the
descent occurs from a different location.</p>
<p>To get Nesterov momentum, you need only add a
<code>mom_type = &quot;nesterov&quot;</code> parameter to a momentum
optimizer:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>            <span class="at">mom_type =</span> <span class="st">&quot;nesterov&quot;</span>)</span></code></pre></div>
<p>I know you’re itching to see whether Nesterov momentum can calm down
the bad case of non-monotone convergence we got with a large classical
momentum value on the Rosenbrock function:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>resc <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>resn <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>             <span class="at">mom_type =</span> <span class="st">&quot;nesterov&quot;</span>, </span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a><span class="co"># Best f found for Nesterov momentum</span></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>resn<span class="sc">$</span>f</span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a><span class="co">#&gt; [1] 1.069263</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a><span class="co"># Best f found for classical momentum</span></span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>resc<span class="sc">$</span>f</span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a><span class="co">#&gt; [1] 2.414765</span></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a><span class="fu">plot</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="fu">log</span>(resn<span class="sc">$</span>progress<span class="sc">$</span>f)))</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a><span class="fu">lines</span>(resn<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resn<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --></p>
<p>The Nesterov momentum is in red. And yes, it’s a lot smoother. On the
other hand, you can see that the cost function increases and then levels
off. Clearly, some tweaking of the correct momentum schedule to use is
needed.</p>
<p>Sutskever also came up with a simplified approximation to the
momentum schedule used in NAG. It doesn’t have a big effect on the run
time, only applies for the default <code>nest_q</code> of 0, and at
early iterations the momentum is much larger. To use it, set the
<code>nest_convex_approx = TRUE</code> when using the
<code>mom_schedule = &quot;nesterov&quot;</code>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, </span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>            <span class="at">mom_schedule =</span> <span class="st">&quot;nsconvex&quot;</span>, <span class="at">nest_convex_approx =</span> <span class="cn">TRUE</span>, </span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>            <span class="at">mom_type =</span> <span class="st">&quot;nesterov&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="line-searches" class="section level2">
<h2>Line Searches</h2>
<p>All the above methods need a line search to find out how far to move
in the gradient descent (i.e. non-momentum) part.</p>
<div id="wolfe-line-search" class="section level3">
<h3>Wolfe Line Search</h3>
<p>The default for all methods mentioned so far is a Wolfe line search
that satisfies the strong Wolfe conditions. Specifically, <a href="https://doi.org/10.1145/192115.192132">More-Thuente line
search</a>, converted from <a href="https://www.cs.umd.edu/users/oleary/software/">Dianne O’Leary’s
Matlab code</a>. This uses cubic interpolation to find a point close to
a minimize along the search direction for each iteration. This requires
a function and gradient evaluation at each candidate point.</p>
<p>If the More-Thuente line search fails to work for some reason,
alternative Wolfe line search method from Carl Edward Rasmussen’s <a href="https://gaussianprocess.org/gpml/code/matlab/doc/">conjugate
gradient routine</a>, Mark Schmidt’s <a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minimization
routines</a>, and an implementation of <a href="https://doi.org/10.1137/030601880">Hager-Zhang</a> line search are
also available (although they are all quite similar):</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;Rasmussen&quot;</span>)</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="co"># Use Mark Schmidt&#39;s minFunc line search </span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;Schmidt&quot;</span>)</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a><span class="co"># Hager-Zhang line search  </span></span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;Hager-Zhang&quot;</span>)</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a><span class="co"># Hager-Zhang can be abbreviated to &quot;HZ&quot;</span></span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;HZ&quot;</span>)</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a><span class="co"># You can explicitly set More-Thuente too</span></span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;More-Thuente&quot;</span>)</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a><span class="co"># More-Thuente can be abbreviated to &quot;MT&quot;</span></span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;MT&quot;</span>)</span></code></pre></div>
<p>A variety of interpolation methods are available in the Matlab
minfunc code, but mize only uses the default: use cubic interpolation
and extrapolation methods, falling back to back-tracking Armijo search
(also using cubic interpolation) if a non-finite value is
encountered.</p>
<p>Default values for the line search tolerance use the advice given in
the Nocedal and Wright book: the <code>c1</code> parameter is set to
<code>1e-4</code>, while <code>c2</code> is set to <code>0.9</code> for
quasi-Newton methods (BFGS and L-BFGS) and <code>0.1</code> for
everything else. For a line search involving the strong Wolfe
conditions, the <code>c2</code> parameter measures how strict the line
search is: the smaller it is (it cannot be set smaller than
<code>c1</code> or greater than 1), the tighter the line search. This is
an important property for some conjugate gradient methods to maintain
their convergence properties.</p>
<p>You may be able to get away with a looser line search (smaller
<code>c2</code>) for some methods, e.g.:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">cg_update =</span> <span class="st">&quot;HZ+&quot;</span>, </span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>            <span class="at">c2 =</span> <span class="fl">0.5</span>, <span class="at">c1 =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<div id="initial-step-size-guess" class="section level4">
<h4>Initial Step Size Guess</h4>
<p>Another important choice is what step size to start each iteration
from. Nocedal and Wright suggest two methods based on the result
achieved for the previous iteration, one involving the ratio of the
slopes at consecutive iterations, and one involving a quadratic
interpolation. By default, for Wolfe line searches other than
<code>&quot;Hager-Zhang&quot;</code>, the quadratic interpolation method is
tried.</p>
<p>To try the slope ratio method, set
<code>step_next_init = &quot;slope&quot;</code>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">step_next_init =</span> <span class="st">&quot;slope&quot;</span>)</span></code></pre></div>
<p>In the <a href="https://doi.org/10.1145/1132973.1132979">CG_DESCENT</a> software,
Hager and Zhang suggest using a “QuadStep” method. In this case, a small
trial step is attempted followed by a quadratic interpolation. If the
resulting quadratic is strongly convex (for a 1D quadratic <span class="math inline">\(Ax^2 + Bx + c\)</span>, this just means <span class="math inline">\(A &gt; 0\)</span>), then the minimizer of the
quadratic is used as the initial guess. Otherwise, a multiple of the
previous step length is used (in <code>mize</code>, the old step length
is doubled). This is the default if <code>line_search = &quot;hz&quot;</code> is
used. To explicitly set it for other methods, use
<code>step_next_init = &quot;hz&quot;</code> (or <code>&quot;hager-zhang&quot;</code>):</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">step_next_init =</span> <span class="st">&quot;hz&quot;</span>, </span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>            <span class="at">line_search =</span> <span class="st">&quot;mt&quot;</span>)</span></code></pre></div>
<p>For the budget-conscious, be aware that using this method always
incurs an extra function evaluation per iteration, even if the quadratic
minimizer isn’t used as the guess.</p>
</div>
<div id="initial-step-size-guess-on-first-iteration" class="section level4">
<h4>Initial Step Size Guess on First Iteration</h4>
<p>This only leaves what to do on the very first iteration, where there
is no previous iteration to look at. You don’t really have much
information at that point apart from the gradient, so all the
implementations I’ve seen use a step size that is effectively inversely
proportional to a norm of the gradient:</p>
<ul>
<li>Carl Edward Rasmussen’s <a href="https://gaussianprocess.org/gpml/code/matlab/doc/">conjugate
gradient routine</a> uses <span class="math inline">\(\frac{1}{1+\left\|
g \right\|_2^2}\)</span></li>
<li><a href="https://scipy.org/">SciPy</a>’s <code>optimize.py</code>
uses <span class="math inline">\(\frac{1}{\left\| g
\right\|_2}\)</span></li>
<li>Mark Schmidt’s <a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minimization
routines</a> uses <span class="math inline">\(\frac{1}{1+\left\| g
\right\|_1}\)</span></li>
<li>Hager and Zhang use a slightly more complicated procedure for their
<a href="https://doi.org/10.1145/1132973.1132979">CG_DESCENT</a>
software: use <span class="math inline">\(\psi_0\frac{\left\|x\right\|_\infty}{\left\|g\right\|_\infty}\)</span>,
where <span class="math inline">\(x\)</span> is the initial parameter
vector and <span class="math inline">\(\psi_0\)</span> is a small
positive value (e.g. 0.01), or if that doesn’t produce a finite positive
value, use <span class="math inline">\(\psi_0\frac{\left|f\right|}{\left\|g\right\|_{2}^{2}}\)</span>
where <span class="math inline">\(\left|f\right|\)</span> is the
absolute value of the function evaluated at <span class="math inline">\(x\)</span>. And if that doesn’t work, just use
1.</li>
</ul>
<p>By default, the Hager-Zhang line search uses the CG_DESCENT method.
For other Wolfe line searches I have arbitrarily plumped for the
Rasmussen method. If you think it will make a huge difference you can
explicitly choose by passing one of <code>&quot;rasmussen&quot;</code>,
<code>&quot;scipy&quot;</code>, <code>&quot;schmidt&quot;</code> or <code>&quot;hz&quot;</code> to the
<code>step0</code> argument, e.g.:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">step0 =</span> <span class="st">&quot;scipy&quot;</span>)</span></code></pre></div>
<p>You may also pass a number to <code>step0</code> and it will be used
as-is:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="co"># An initial guess of 1 for the step length isn&#39;t bad for L-BFGS</span></span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">step0 =</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">&quot;L-BFGS&quot;</span>)</span></code></pre></div>
<p>The L-BFGS and BFGS methods will also attempt the “natural” Newton
step length of 1, in the way suggested in Nocedal and Wright. If you
don’t want to do this for some reason, it can be explicitly turned off
or on via the <code>try_newton_step</code> parameter:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">try_newton_step =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>For every other method this is off by default anyway with no
particular reason to turn it on.</p>
<p>All Wolfe line searches except <code>Hager-Zhang</code> use the
strong curvature condition to determine when to terminate the line
search. The Hager-Zhang line search uses the standard curvature
conditions by default. It also uses an approximation to the Armijo
sufficient descent conditions which may prevent premature termination of
a line search when the minimizer lies close to the initial step size
under some circumstances. Use of these variations on the Wolfe
conditions can be applied to any of the Wolfe line searches by supplying
the <code>strong_curvature</code> and <code>approx_armijo</code>
options:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="co"># Rasmussen line search with standard Wolfe conditions</span></span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;Rasmussen&quot;</span>,</span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a>            <span class="at">strong_curvature =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-4"><a href="#cb38-4" tabindex="-1"></a><span class="co"># Hager-Zhang with strong Wolfe conditions</span></span>
<span id="cb38-5"><a href="#cb38-5" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;HZ&quot;</span>,</span>
<span id="cb38-6"><a href="#cb38-6" tabindex="-1"></a>            <span class="at">strong_curvature =</span> <span class="cn">TRUE</span>, <span class="at">approx_armijo =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-7"><a href="#cb38-7" tabindex="-1"></a><span class="co"># More-Thuente with approx Armijo conditions</span></span>
<span id="cb38-8"><a href="#cb38-8" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;MT&quot;</span>,</span>
<span id="cb38-9"><a href="#cb38-9" tabindex="-1"></a>            <span class="at">approx_armijo =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>It’s not obvious to me that any proof of convergence that exists for
a given line search method will hold up when the curvature and Armijo
conditions are altered, so be careful when using specifying these
options.</p>
</div>
</div>
<div id="other-line-searches" class="section level3">
<h3>Other Line Searches</h3>
<p>The Wolfe line search is your best bet, but there are some
alternatives (of somewhat dubious value).</p>
<p>You can set a constant value for the step size. But note that the
total step size is actually determined by the product of the step size
and the length of the gradient vector, so you may want to set
<code>norm_direction</code> too:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;SD&quot;</span>, <span class="at">line_search =</span> <span class="st">&quot;constant&quot;</span>,</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a>            <span class="at">norm_direction =</span> <span class="cn">TRUE</span>, <span class="at">step0 =</span> <span class="fl">0.01</span>)</span></code></pre></div>
<p>There is also a backtracking line search, which repeatedly reduces
the step size starting from <code>step0</code>, until the sufficient
decrease condition (also known as the Armijo condition) of the Wolfe
line search conditions is fulfilled. This is controlled by
<code>c1</code>, the larger the value, the larger the decrease in the
function value is needed for a step size to be accepted. By default, the
cubic interpolation method from <a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a>
is used to find the step size.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">line_search =</span> <span class="st">&quot;backtracking&quot;</span>, <span class="at">step0 =</span> <span class="dv">1</span>, </span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a>            <span class="at">c1 =</span> <span class="fl">0.1</span>)</span></code></pre></div>
<p>You will often see a simple fixed step size reduction used in
combination with this line search, mainly due to ease of implementation.
Cubic interpolation is probably always preferable, but if you want to
use a constant step size reduction, provide a non-<code>NULL</code>
value to <code>step_down</code>. The step size will be multiplied by
this value so it should be a value between zero and one. Set it to
<code>0.5</code>, for example, to halve the step size while
backtracking:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">line_search =</span> <span class="st">&quot;backtracking&quot;</span>,</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a>            <span class="at">step0 =</span> <span class="dv">1</span>, <span class="at">c1 =</span> <span class="fl">0.1</span>, <span class="at">step_down =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p>A similar approach is the <code>&quot;bold driver&quot;</code>, which also
backtracks, but starts from the step size found for the last iteration,
increased by a factor <code>step_up</code>.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="co"># increase step size by 10%, but reduce by 50%</span></span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">line_search =</span> <span class="st">&quot;bold&quot;</span>,</span>
<span id="cb42-3"><a href="#cb42-3" tabindex="-1"></a>            <span class="at">step0 =</span> <span class="dv">1</span>, <span class="at">step_down =</span> <span class="fl">0.5</span>, <span class="at">step_up =</span> <span class="fl">1.1</span>)</span></code></pre></div>
<p>Unlike backtracking line search, the bold driver accepts any function
decrease, and does not use <code>c1</code>.</p>
</div>
<div id="maximum-function-or-gradient-evaluations-per-line-search" class="section level3">
<h3>Maximum function or gradient evaluations per line search</h3>
<p>It’s a good idea to specify a maximum number of function evaluations
that can be spent in any one line search. This guards against the
possibility of poorly chosen parameters, badly behaved functions or
other numerical issues causing the line search to fail to converge. By
default, no more than 20 function evaluations are allowed per line
search. The <code>ls_max_fn</code>, <code>ls_max_gr</code> and
<code>ls_max_fg</code> can be used to control this:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="co"># No more than 10 gradient evaluations allowed per line search</span></span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">ls_max_gr =</span> <span class="dv">10</span>)</span></code></pre></div>
</div>
</div>
<div id="adaptive-learning-rate-methods" class="section level2">
<h2>Adaptive Learning Rate Methods</h2>
<p>So far, the <code>method</code> and <code>line_search</code>
arguments can be freely mixed and matched. However, some methods choose
both the direction and the step size directly. The <a href="https://doi.org/10.1016/0893-6080(88)90003-2">Delta-Bar-Delta
method</a> has its roots in neural network research but was also used in
the currently popular <a href="https://lvdmaaten.github.io/tsne/">t-Distributed Stochastic
Neighbor Embedding</a> data visualization method.</p>
<p>The way DBD works is to consider each component of the gradient and a
weighted average of previous iterates gradients. If the sign has changed
for that component, that suggests the minimum has been skipped, so the
step size (where each component is initialized with value
<code>step0</code>) is reduced (by a factor <code>step_down</code>) in
that direction. Otherwise the step size is increased (by a factor
<code>step_up</code>). The weighting of the average of previous
gradients is controlled by <code>dbd_theta</code>, which varies between
0 and 1.</p>
<p>The same line search parameters that control the bold driver method
can be used in DBD:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;DBD&quot;</span>,</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a>            <span class="at">step0 =</span> <span class="st">&quot;rasmussen&quot;</span>, <span class="at">step_down =</span> <span class="fl">0.5</span>, <span class="at">step_up =</span> <span class="fl">1.1</span>,</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a>            <span class="at">dbd_weight =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p>One extra parameter is available: <code>step_up_fn</code>. In the
original DBD paper, when the new step size is to be increased for a
component, it is done by adding the value of <code>step_up</code> to the
current value of the step size. <a href="https://doi.org/10.1109/IJCNN.1998.687205">Janet and
co-workers</a> noted that when the step sizes are small compared to the
value of <code>step_up</code> this can lead to far too large a step size
increase. Instead, they suggest multiplying by <code>step_up</code> so
that a percentage increase in the step size is achieved. This is the
default behavior in <code>mize</code>, but the t-SNE optimization method
uses the addition method. To get this behavior, set the
<code>step_up_fun</code> value to <code>&quot;+&quot;</code>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;DBD&quot;</span>,</span>
<span id="cb45-2"><a href="#cb45-2" tabindex="-1"></a>            <span class="at">step0 =</span> <span class="st">&quot;rasmussen&quot;</span>, <span class="at">step_down =</span> <span class="fl">0.8</span>, <span class="at">step_up =</span> <span class="fl">0.2</span>,</span>
<span id="cb45-3"><a href="#cb45-3" tabindex="-1"></a>            <span class="at">step_up_fun =</span> <span class="st">&quot;+&quot;</span>)</span></code></pre></div>
<p>The DBD method can be used with momentum. If it is, then the
<code>dbd_weight</code> parameter is ignored and instead of storing its
own history of weighted gradients, the update vector stored by the
momentum routines is used instead. This is not mentioned in the original
paper, but is used in the t-SNE implementation.</p>
<p>The DBD method is interesting in that it uses no function evaluations
at all in its optimization. It doesn’t even avail itself of the values
of any of the gradient components, only their signs. However, as a
result, it’s really, really easy for DBD to diverge horribly and
quickly. You will therefore want to ensure that you are checking the
convergence. This is one of the methods where using the
<code>grad_tol</code> or <code>ginf_tol</code> is a better idea than
<code>abs_tol</code> and <code>rel_tol</code>, because it provides some
check on divergence without calculating the function ever iteration only
for convergence checking purposes.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="co"># DBD with rel_tol and abs_tol is explicitly set</span></span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;DBD&quot;</span>,</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a>             <span class="at">step0 =</span> <span class="st">&quot;rasmussen&quot;</span>, <span class="at">step_down =</span> <span class="fl">0.8</span>, <span class="at">step_up =</span> <span class="fl">0.2</span>,</span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a>             <span class="at">step_up_fun =</span> <span class="st">&quot;+&quot;</span>, <span class="at">rel_tol =</span> <span class="fl">1e-8</span>, <span class="at">abs_tol =</span> <span class="fl">1e-8</span>)</span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a><span class="co"># 10 gradient calculations as expected</span></span>
<span id="cb46-6"><a href="#cb46-6" tabindex="-1"></a>res<span class="sc">$</span>ng</span>
<span id="cb46-7"><a href="#cb46-7" tabindex="-1"></a><span class="co">#&gt; [1] 10</span></span>
<span id="cb46-8"><a href="#cb46-8" tabindex="-1"></a><span class="co"># But 10 function calculations too, only used in the tolerance check</span></span>
<span id="cb46-9"><a href="#cb46-9" tabindex="-1"></a>res<span class="sc">$</span>nf</span>
<span id="cb46-10"><a href="#cb46-10" tabindex="-1"></a><span class="co">#&gt; [1] 10</span></span>
<span id="cb46-11"><a href="#cb46-11" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" tabindex="-1"></a><span class="co"># Turn off the rel_tol and abs_tol and let max_iter handle termination</span></span>
<span id="cb46-13"><a href="#cb46-13" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">&quot;DBD&quot;</span>,</span>
<span id="cb46-14"><a href="#cb46-14" tabindex="-1"></a>            <span class="at">step0 =</span> <span class="st">&quot;rasmussen&quot;</span>, <span class="at">step_down =</span> <span class="fl">0.8</span>, <span class="at">step_up =</span> <span class="fl">0.2</span>,</span>
<span id="cb46-15"><a href="#cb46-15" tabindex="-1"></a>            <span class="at">step_up_fun =</span> <span class="st">&quot;+&quot;</span>, <span class="at">rel_tol =</span> <span class="cn">NULL</span>, <span class="at">abs_tol =</span> <span class="cn">NULL</span>,</span>
<span id="cb46-16"><a href="#cb46-16" tabindex="-1"></a>            <span class="at">grad_tol =</span> <span class="fl">1e-5</span>)</span>
<span id="cb46-17"><a href="#cb46-17" tabindex="-1"></a><span class="co"># 11 gradient calculations</span></span>
<span id="cb46-18"><a href="#cb46-18" tabindex="-1"></a>res<span class="sc">$</span>ng</span>
<span id="cb46-19"><a href="#cb46-19" tabindex="-1"></a><span class="co">#&gt; [1] 11</span></span>
<span id="cb46-20"><a href="#cb46-20" tabindex="-1"></a><span class="co"># Only one function evalation needed (to calculate res$f)</span></span>
<span id="cb46-21"><a href="#cb46-21" tabindex="-1"></a>res<span class="sc">$</span>nf</span>
<span id="cb46-22"><a href="#cb46-22" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>Using <code>grad_tol</code> results in one extra gradient calculation
(because it using gradients for the convergence check), but only one
function evaluation. The more iterations this runs for, the better trade
off this is, although in the event of a diverging optimization, the
“best” <code>par</code> that is returned may not be as good as would
have been returned if we were also using function evaluations.</p>
</div>
<div id="adaptive-restart" class="section level2">
<h2>Adaptive Restart</h2>
<p><a href="https://arxiv.org/abs/1204.3982">O’Donoghue and Candes</a>
described a restart scheme for NAG as a cure for the non-monotonic
convergence. They suggest using either a function-based check (did the
function value after the gradient descent stage decrease between
iterations?) or a gradient-based check (is the momentum direction a
descent direction?). If the check fails, then the momentum is restarted:
the previous update vector is discarded and the momentum set back to
zero.</p>
<p><code>mize</code> allows these checks to be used to restart any
momentum scheme, not just NAG. The only change is that the
function-based check is carried out at the beginning and end of each
iteration, rather than at the end of the gradient descent stage.
Restarting is applied using the <code>restart</code> argument, supplying
either <code>fn</code> or <code>gr</code> for function-based and
gradient-based restarting, respectively:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a>resc <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb47-2"><a href="#cb47-2" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>)</span>
<span id="cb47-3"><a href="#cb47-3" tabindex="-1"></a>resf <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb47-4"><a href="#cb47-4" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>, <span class="at">restart =</span> <span class="st">&quot;fn&quot;</span>)</span>
<span id="cb47-5"><a href="#cb47-5" tabindex="-1"></a>resg <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb47-6"><a href="#cb47-6" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>, <span class="at">restart =</span> <span class="st">&quot;gr&quot;</span>)</span>
<span id="cb47-7"><a href="#cb47-7" tabindex="-1"></a><span class="fu">plot</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb47-8"><a href="#cb47-8" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="fu">log</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>f),</span>
<span id="cb47-9"><a href="#cb47-9" tabindex="-1"></a>                  <span class="fu">log</span>(resg<span class="sc">$</span>progress<span class="sc">$</span>f)))</span>
<span id="cb47-10"><a href="#cb47-10" tabindex="-1"></a><span class="fu">lines</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb47-11"><a href="#cb47-11" tabindex="-1"></a><span class="fu">lines</span>(resg<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resg<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASAAAAEgCAMAAAAjXV6yAAAAbFBMVEUAAAAAADoAAGYAAP8AOpAAZrY6AAA6ADo6AGY6kNtmAABmADpmAGZmOpBmkJBmtrZmtv+QOgCQZgCQkGaQtpCQ2/+2ZgC2tma2/7a2///bkDrb25Db/7bb////AAD/tmb/25D//7b//9v///+TYWRjAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAMvElEQVR4nO2di3rkJhKF8TDtZDqZjb2Jeze9sdttvf87rqAAgS4cBOhm1/mSbgmVkPofKC5CZdGwohJb38DexYCAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAMCIgBATEgIAYExICAGBAQAwJiQEAdoJsgPW14NzuUAXQ/ixNtfTyLb39vdz+7EwG6/+4zCfe+uNgHAZkSdH66//ay8a3sUxbQDwY0LlPFrsKJXXQg54O4BI2LnTSQBXT5/koVjfuJoawPemgr2K39eH9kQoFsK3ZSfWjVmdZFieVEgC7C14+N72lXcv0gqmFcgvoyPujy7W+qYTcuP6FcKyYEtWPMJ1TlfpA4jLYCVDe75ZQBSDnqK1W0Fa67tTIAtc3X++OpuZxWue7Wmg/ofn5qm7D2/6LR/FKAcrxHPMPZhgpQ29g3110BGoGyFaC2bt3P31/1oGOF66blNpqdTZTzMgvNs5y0eHih3mK+agKaqkxbAaqietlN+xoGpF0Pvoacl2dovlA/6OOZfOakI1/Dg24FKKEf5EZqk0PaKoBQJua4lOY7LdfQbJF+0Mezw3KdKGg1AME8lIHcBBDoBykToymMFQDhLLYBlNAPWqUEJeSgAUkLyHKKSq7TD7raZx4L+qD9AkpRC1FrsqkrBpSSAVUxaQGl1DH5afpBywBSpmFJywHUFo/vr5esKdf8obaI7k6fQ4DM1sRp0tuS5YBuDy9X5aSLJqW3ACTWAaSaKNU2rTDdEZm8mAtItHyEnBi4dUhqAFKdHAUo0lG0Ljoy2FgBUAtGECAFh9xv+y2Gp9cFZEtQ7MHhxzMqXmmAxPh22unS/Lc2IOODrtHlHXC6KK0RKgUkpAXUUGkygMQEINnUAEQ16CG+jOoGlsckttKdmftNuglMBWT6iRaQOlMIi150ltKdUgNQDeUBMv2DWYCE+k8oNn1A0skD5LpNM240NPQGWgVK7ecZLATIFaJZgMixCGrpO0DahetsKwPyhuoFmgHIFR3RHZhTxajW6DMMIKG9k1gIUOEDsfTrmuZG2B0xAxBVnHFABIVy0TmpOmhOa6qUoHgPZ2Z2wMRSCf0ROF3/xAlACod22aIJAJFhMxiy7dhJZwOS9Ck7QHaQ4XDJxvlq48ipS9B8cUCmAtGhEFC7TXWrGJA3jijw1hmA/COx06X5kmOAmggg1dSJCiWIZgnbnmDJgD7dSY/Y5wByR2U3eu8AeU6pGJAdRajhRv4aoeUASfstVfkzPGR32NszvU7tgDpAQvq55/eD2ta+oMFfGZB32E+YAuSdkd+TViVoh4Ck25AWUHhcTgEyrT4BcpOeJT6oZIXHSoCGjzKGgIQPSG/pblM+IGrH2sJzKegq4utGLCKAZLclu0/fQPYANfUB1dA+AFFusvnKgAYGchxQUxHQ+yOeMJuR3XyLBQB1/UbbKSp10vEp1xnZZVgMAcnu6WCXFO57yWFiB0h/FAPymvnUc2PZ5VgMAEn9IZssQHrqwwPUFALyOoqp58aym2NhJ0jHAdFsjmfcNMmAmoqAtilBmswbbQ7Olt2X9M5ommxA3eTBIj6IRrG3sjWKgYVhQ9t7B4RbMQ1Il7DJCex5gGQT7iQBGp6pE+QkIPqQ5YCgFCCDJn+FmWchgwNyUP28b9lLDU81KXKQEgJqigAlPPZRgMxr4z1XHl/+EryI0lnI0EouAYiu7/ZKACU89skqQQE0H5Dvf/R+3z+5Ddcb6pJl05dcGlBC+06zsqfGuutodmZf9A/bhD6fGKCdlKCUxz56pWfk1WgR3fV7OrJ/qowB6icPzl6hBFVRkN3QI4npAtQHJLst2QzSgySbIgcpI7e3I0AjWeuHMP6yAk9yBUDRm4OGupIVxl0Qo5temtBF55CAKDLFtWy+IxWQHB6TOwf08UzNfFnsDjGyZWXGpPMB9c0GaSZFDlKmldGKmRBdlUbzgxvQTNSs3jQg1+EZa9jDveH5KSljN4o0KEGVRvP9G5AmNQpIuG7zsGsY7g3PT0kZu1EkfzSvCFWZUZQjBcgcngbUmOW9ZLo/QAmLoJOzGwKS9vCKgKLath8kB3ze7GEMSJqdLj20GkkbT0m40ZqGc7JzgEyv0Ov1HxZQpTCBfUCmV+jGFRiQPdJZhLZyJG08JeFG0w1rhQkMARGYUkA9q+jRZM0EVC1MoCVjvgiQNzCNAmqa3QKqFiawB4g+PwGgamECEwBR0si54ZExi1j6TM31QbXCBAaApNnJAjQldDxR81uxOmECIaAIhukjA6tybTxhZh8mmx0GNJpd504+CSDlnksjlY5Ou0v/uN6ZBgQBoOOJygBEzdcSMcykf1zvjAN6e3urBgAp/8HhAstfpH9c7wyfaRi97ReQW/6yKKAGAGre5NSRusqoYjRV9v4Yc0J5IbrkYOeQgBIWcWaG6JKDnWlA+61iCcoNsCQHO58U0HSIrhnRX6T+iACaPlRTGYBwmMAqIbqk/jgioKQwgZVCdB0RUFK45Fohuo4KaLVwyUcEtGq45EMCWjNccrSZX4XQHqY7IooWEwaEAOlR/cLKXGGWGyZw7nWho1mcUN4Ks9XCBGJPvDShvOmOlcIEHhNQQpjAetdNaMsXJpRfgmqtUSzV7gAlhQmsd12ohVuyzFZshbeek7U/QKteF2tvgNYLE5ioZetY/mOfla6boJ0BWi1MYLIWLUIFy4D38+ez9gVo5eumaMki9CkALVmE5i7B+92vV+HeMtdN0oJFaG4Jaj2QmUr8eC7xQpUL5H4ANRRyIh5mskpoinlajNBCT1YrhKaYpzdS5VybBQEVh6bI0QKMCvpB05VsOjTF/OturSwfhGK5blaCFlDOYBXGcs0ITbFbFa1RjP9xlrHQFDOWv+xEBWsUy2K5HkazAaXHcsUzRxNXn5W8nPE8P9Cfck2I5fplAaWKAQF9WUCpsVy/KqAqsVyjd3BsQHUicUbv4NiA6rzMEr2DYwPiEoSMK/qgz6SMVuxr6TCjy63EgIAYEJB5LlYl/NSnFJcgIAYExICAGBAQAwJiQEAMCKg+oFvigI6W9DvryGkueiE2fv9V9eJ0gIgfaWcgVQekIqHdEu7lpt8MdtaR0676UeVTivH9rLq5H8/tsat6BJySPVBtQDStFHu3nKRfTuusI6eZ6IWnBOMbjQNogcVVhWXD2SPVBuRuDthdv/+7BeSsI6c5QND4Jn5486FtgUnJHqk6oF9UOYbztq2Z8kHOOnaarWIpxl7ixTNMvKsx1QZEFR1Vd1XkdUQwax09zTjYFOOOgZogTcs+rm0A2RfTkn6BehSuwhrNAnSzPnpvgJIKszZKrWKdI5lRxWiCfY9VLMkdXs3U01OKF3X/+knGlGgiQe3RSac3qJfEZp5+3C2t0SZA9tHMHpv59C7ZJbWjaH1QivGN+kF28cAOO4q6/swYajjryGkXO3DAxhqQqcHKIiX7uHiwCsSAgBgQEAMCYkBADAiIAQExICAGBMSAgBgQEAMCYkBADAiIAQExICAGBMSAgBgQEAMCYkBADAiIAQExICAGBMSAgPYC6P6z/FXZetl42gkg9/i9bjbX8lz3Aejy7b8//1f2d8rHsin8CwZauwD0/vjU1o2yYPFj2dSIcLwSoPtvf6olzFdTBfQL1nrRl1Axv+iXqfRf/nrUFsb+ZqtMW3ce/nx4mcjGfLts7uc/znr9WoX3J9cCdNZ/SfrhRa9u0ovG1MpeldDWg/boPwToUdAf8SJ72jrRGqqbaAGNZmO/XTb3s1pqr8yPU4KUNzCRPLsgac5HqH/rU2PXhtm/f0IL59zqxIv6xWPZuMWZNhttocrTkQA92eWY+sbpJ713DlW9VmyjNN7Mvz3ttZ9EwCYPsrHfLhtt1n4cDpBd3Eqv49i182TQele3XDcA1G5eQ0D9bNy3zeawgIJVghdbdcjg52tLZwzQRAnysvG+KZujAgrvV/8A8kHqp/981c5GL9dVPuhp4IOs1x1kY79dNkcFpJsf9U/tVuaqhI/nk25+/jmr9t68uEL2I63YWDb222VzWEC6A6Oqy83GmaF+EDmRk6pO/3rU3sTY+/2gb/8xzmWYjf222RwQEJTpKMaGGymvWnzawarRBCCdXGNglaFDAKKWfZvQWPsCtEMxICAGBMSAgBgQEAMCYkBADAiIAQExICAGBMSAgBgQEAMCYkBADAiIAQExICAGBMSAgBgQ0P8BUqBi40vS2C8AAAAASUVORK5CYII=" /><!-- --></p>
<p>Adaptive restart (red and blue lines) certainly seems to help here,
although with either gradient or function-based restart seeming equally
effective.</p>
<p>The only downside to adaptive restart is that you may want to choose
the type of restart to not require more overhead of function and
gradient calls than necessary. In practice, the gradient-based restart
is economical because in most cases, the gradient calculated at the end
of the iteration can be re-used for the gradient descent at the start of
the next iteration. The one exception here is when using nesterov
momentum, where the gradients aren’t calculated in the right location to
be of use. In that case, you are better off using function-based
restarting, although if you aren’t checking convergence with a function
calculation or using a line search method that makes use of the function
at the starting point, you will obviously incur the overhead of the
function call. This is one reason to favour using the NAG method
directly rather than emulating it with Nesterov momentum.</p>
<p>There is also a <code>restart_wait</code> parameter. This determines
how many iterations to wait between attempted restarts. By default,
using the value from the paper of Su and co-workers, we wait 10
iterations before allowing another restart. Making the wait time too
short may lead to premature convergence:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>resfw <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>, <span class="at">restart =</span> <span class="st">&quot;fn&quot;</span>, <span class="at">restart_wait =</span> <span class="dv">1</span>)</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a>resgw <span class="ot">&lt;-</span> <span class="fu">mize</span>(rb0, rb_fg, <span class="at">max_iter =</span> <span class="dv">100</span>, <span class="at">method =</span> <span class="st">&quot;MOM&quot;</span>, <span class="at">mom_schedule =</span> <span class="fl">0.9</span>, </span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a>             <span class="at">store_progress =</span> <span class="cn">TRUE</span>, <span class="at">restart =</span> <span class="st">&quot;gr&quot;</span>, <span class="at">restart_wait =</span> <span class="dv">1</span>)</span>
<span id="cb48-5"><a href="#cb48-5" tabindex="-1"></a><span class="fu">plot</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb48-6"><a href="#cb48-6" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">log</span>(resc<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="fu">log</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>f),</span>
<span id="cb48-7"><a href="#cb48-7" tabindex="-1"></a>                  <span class="fu">log</span>(resg<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="fu">log</span>(resfw<span class="sc">$</span>progress<span class="sc">$</span>f),</span>
<span id="cb48-8"><a href="#cb48-8" tabindex="-1"></a>                  <span class="fu">log</span>(resgw<span class="sc">$</span>progress<span class="sc">$</span>f)))</span>
<span id="cb48-9"><a href="#cb48-9" tabindex="-1"></a><span class="fu">lines</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resf<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb48-10"><a href="#cb48-10" tabindex="-1"></a><span class="fu">lines</span>(resfw<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resfw<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb48-11"><a href="#cb48-11" tabindex="-1"></a><span class="fu">lines</span>(resgw<span class="sc">$</span>progress<span class="sc">$</span>nf, <span class="fu">log</span>(resgw<span class="sc">$</span>progress<span class="sc">$</span>f), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" /><!-- --></p>
<p>Here, reducing the wait time to 1 had a great effect on gradient
restarting (the orange line), it substantially outperforms the other
methods. On the other hand, function restarting more often (the blue
line) led to worse performance.</p>
<p>There is also a <code>&quot;speed&quot;</code> restart type, related to a
restart strategy introduced by <a href="https://papers.nips.cc/paper/5322-a-differential-equation-for-modeling-nesterovs-accelerated-gradient-method-theory-and-insights">Su
and co-workers</a>, which restarts if the update vector doesn’t increase
in length (as measured by Euclidean 2-norm) between iterations. This
seems less performant in their experiments than gradient-based
restarting, but might provide more stability under some
circumstances.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
